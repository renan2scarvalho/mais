{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trabalhando pela universaliza\u00e7\u00e3o do uso de dados no Brasil Base dos Dados Mais A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados no Brasil. Acreditamos que a dist\u00e2ncia entre uma pessoa e uma an\u00e1lise deveria ser apenas uma boa ideia. Para realizar essa vis\u00e3o, n\u00f3s constru\u00edmos a Base dos Dados Mais (BD+): um reposit\u00f3rio integrado de dados. Essa ferramenta traz acesso, rapidez, escala, facilidade, economia, curadoria, e transpar\u00eancia ao cen\u00e1rio de dados no Brasil. Todos os nossos dados ficam organizados e dispon\u00edveis na nuvem dentro da ferramenta da Google chamada BigQuery . Uma simples consulta de SQL \u00e9 o suficiente para cruzamento de bases que voc\u00ea desejar. Sem precisar procurar, baixar, tratar, comprar um servidor e subir clusters na nuvem. Por que o BigQuery? Acesso : \u00c9 poss\u00edvel deixar os dados p\u00fablicos, i.e., qualquer pessoa com uma conta no Google Cloud pode fazer uma query na base, quando quiser. Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Facilidade : Voc\u00ea pode cruzar tabelas tratadas e atualizadas num s\u00f3 lugar. Economia : O custo \u00e9 praticamente zero para usu\u00e1rios - 1 TB gratuito por m\u00eas para usar como quiser . Depois disso, s\u00e3o cobrados somente 5 d\u00f3lares por TB de dados que sua query percorrer. Clique para acessar o projeto no BigQuery Quick Start Acesse os dados direto pelo BigQuery \ud83d\udd0d Acesse os dados pelo seu computador (CLI/API) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Como citar o projeto O projeto est\u00e1 licenciado sob a Licen\u00e7a Hipocr\u00e1tica . Sempre que usar os dados cite a fonte como: Carabetta, J.; Dahis, R.; Israel, F.; Scovino, F. (2020) Base dos Dados Mais: Reposit\u00f3rio de Dados. Github - https://github.com/basedosdados/mais. Idiomas Documenta\u00e7\u00e3o est\u00e1 em portugu\u00eas (quando poss\u00edvel), c\u00f3digo e configura\u00e7\u00f5es est\u00e3o em ingl\u00eas.","title":"Introdu\u00e7\u00e3o"},{"location":"#base-dos-dados-mais","text":"A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados no Brasil. Acreditamos que a dist\u00e2ncia entre uma pessoa e uma an\u00e1lise deveria ser apenas uma boa ideia. Para realizar essa vis\u00e3o, n\u00f3s constru\u00edmos a Base dos Dados Mais (BD+): um reposit\u00f3rio integrado de dados. Essa ferramenta traz acesso, rapidez, escala, facilidade, economia, curadoria, e transpar\u00eancia ao cen\u00e1rio de dados no Brasil. Todos os nossos dados ficam organizados e dispon\u00edveis na nuvem dentro da ferramenta da Google chamada BigQuery . Uma simples consulta de SQL \u00e9 o suficiente para cruzamento de bases que voc\u00ea desejar. Sem precisar procurar, baixar, tratar, comprar um servidor e subir clusters na nuvem.","title":"Base dos Dados Mais"},{"location":"#por-que-o-bigquery","text":"Acesso : \u00c9 poss\u00edvel deixar os dados p\u00fablicos, i.e., qualquer pessoa com uma conta no Google Cloud pode fazer uma query na base, quando quiser. Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Facilidade : Voc\u00ea pode cruzar tabelas tratadas e atualizadas num s\u00f3 lugar. Economia : O custo \u00e9 praticamente zero para usu\u00e1rios - 1 TB gratuito por m\u00eas para usar como quiser . Depois disso, s\u00e3o cobrados somente 5 d\u00f3lares por TB de dados que sua query percorrer. Clique para acessar o projeto no BigQuery","title":"Por que o BigQuery?"},{"location":"#quick-start","text":"Acesse os dados direto pelo BigQuery \ud83d\udd0d Acesse os dados pelo seu computador (CLI/API) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb","title":"Quick Start"},{"location":"#como-citar-o-projeto","text":"O projeto est\u00e1 licenciado sob a Licen\u00e7a Hipocr\u00e1tica . Sempre que usar os dados cite a fonte como: Carabetta, J.; Dahis, R.; Israel, F.; Scovino, F. (2020) Base dos Dados Mais: Reposit\u00f3rio de Dados. Github - https://github.com/basedosdados/mais.","title":"Como citar o projeto"},{"location":"#idiomas","text":"Documenta\u00e7\u00e3o est\u00e1 em portugu\u00eas (quando poss\u00edvel), c\u00f3digo e configura\u00e7\u00f5es est\u00e3o em ingl\u00eas.","title":"Idiomas"},{"location":"access_data_bq/","text":"Como usar via BigQuery Ao clicar no bot\u00e3o voc\u00ea ser\u00e1 redirecionado para logar na sua conta ou criar uma antes de acessar o projeto. Clique para acessar o projeto no BigQuery Na sua tela dever\u00e1 aparecer o projeto fixado no menu lateral esquerdo, como na imagem abaixo. Criando uma conta no BigQuery \u00c9 preciso, basicamente, ter uma conta Google para acessar o BigQuery. O site deve solicitar que voc\u00ea crie um projeto qualquer no seu BigQuery antes de acessar os nossos dados - n\u00e3o se preocupe, n\u00e3o \u00e9 pago! O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais sobre o Sandbox aqui . Acessando o projeto Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tables est\u00e3o organizadas em datasets Cada table pertence a um \u00fanico dataset Caso n\u00e3o apare\u00e7am as tabelas nos datasets do projeto na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina. Explorando os dados Exemplo: Qual a evolu\u00e7\u00e3o do PIB per capita de todos os munic\u00edpios? \ud83d\udcc8 O BigQuery utiliza SQL como linguagem nativa. Leia mais sobre a sintaxe utilizada aqui . Rode a query abaixo no Query Editor/Editor de consultas e obtenha o cruzamento das tabelas de popula\u00e7\u00e3o e PIB do IBGE com o resultado anual desde 1991. SELECT pib . id_municipio , pop . ano , pib . PIB / pop . populacao * 1000 as pib_per_capita FROM ` basedosdados . br_ibge_pib . municipios ` as pib JOIN ` basedosdados . br_ibge_populacao . municipios ` as pop ON pib . id_municipio = pop . id_municipio Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios. Entenda os dados O BigQuery possui j\u00e1 um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais na se\u00e7\u00e3o de Nomenclatura . Metadados Clicando num dataset ou table voc\u00ea j\u00e1 consegue ver toda a estrutura e descri\u00e7\u00e3o das colunas, e pode acessar tamb\u00e9m os detalhes de tratamento e publica\u00e7\u00e3o, para entender melhor os dados.","title":"BigQuery"},{"location":"access_data_bq/#como-usar-via-bigquery","text":"Ao clicar no bot\u00e3o voc\u00ea ser\u00e1 redirecionado para logar na sua conta ou criar uma antes de acessar o projeto. Clique para acessar o projeto no BigQuery Na sua tela dever\u00e1 aparecer o projeto fixado no menu lateral esquerdo, como na imagem abaixo.","title":"Como usar via BigQuery"},{"location":"access_data_bq/#criando-uma-conta-no-bigquery","text":"\u00c9 preciso, basicamente, ter uma conta Google para acessar o BigQuery. O site deve solicitar que voc\u00ea crie um projeto qualquer no seu BigQuery antes de acessar os nossos dados - n\u00e3o se preocupe, n\u00e3o \u00e9 pago! O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais sobre o Sandbox aqui .","title":"Criando uma conta no BigQuery"},{"location":"access_data_bq/#acessando-o-projeto","text":"Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tables est\u00e3o organizadas em datasets Cada table pertence a um \u00fanico dataset Caso n\u00e3o apare\u00e7am as tabelas nos datasets do projeto na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina.","title":"Acessando o projeto"},{"location":"access_data_bq/#explorando-os-dados","text":"","title":"Explorando os dados"},{"location":"access_data_bq/#exemplo-qual-a-evolucao-do-pib-per-capita-de-todos-os-municipios","text":"O BigQuery utiliza SQL como linguagem nativa. Leia mais sobre a sintaxe utilizada aqui . Rode a query abaixo no Query Editor/Editor de consultas e obtenha o cruzamento das tabelas de popula\u00e7\u00e3o e PIB do IBGE com o resultado anual desde 1991. SELECT pib . id_municipio , pop . ano , pib . PIB / pop . populacao * 1000 as pib_per_capita FROM ` basedosdados . br_ibge_pib . municipios ` as pib JOIN ` basedosdados . br_ibge_populacao . municipios ` as pop ON pib . id_municipio = pop . id_municipio Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios.","title":"Exemplo: Qual a evolu\u00e7\u00e3o do PIB per capita de todos os munic\u00edpios? \ud83d\udcc8"},{"location":"access_data_bq/#entenda-os-dados","text":"O BigQuery possui j\u00e1 um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais na se\u00e7\u00e3o de Nomenclatura .","title":"Entenda os dados"},{"location":"access_data_bq/#metadados","text":"Clicando num dataset ou table voc\u00ea j\u00e1 consegue ver toda a estrutura e descri\u00e7\u00e3o das colunas, e pode acessar tamb\u00e9m os detalhes de tratamento e publica\u00e7\u00e3o, para entender melhor os dados.","title":"Metadados"},{"location":"access_data_local/","text":"Como acessar os dados localmente $ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados Instalando a aplica\u00e7\u00e3o CLI pip install basedosdados Python pip install basedosdados R Ainda n\u00e3o temos suporte oficial para R, mas recomendamos o pacote bigrquery . install.packages ( \"bigrquery\" ) Seja a primeira pessoa a contribuir (veja Issue #82 no GitHub)! Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)! Criando um projeto no Google Cloud Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico. Fazendo queries Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipios' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"bigrquery\" )) install.packages ( \"bigrquery\" ) library ( \"bigrquery\" ) billing_project_id = \"<YOUR_PROJECT_ID>\" pib_per_capita = \"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" d <- bq_table_download ( bq_project_query ( billing_project_id , pib_per_capita ), page_size = 500 , bigint = \"integer64\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Local"},{"location":"access_data_local/#como-acessar-os-dados-localmente","text":"$ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados","title":"Como acessar os dados localmente"},{"location":"access_data_local/#instalando-a-aplicacao","text":"CLI pip install basedosdados Python pip install basedosdados R Ainda n\u00e3o temos suporte oficial para R, mas recomendamos o pacote bigrquery . install.packages ( \"bigrquery\" ) Seja a primeira pessoa a contribuir (veja Issue #82 no GitHub)! Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Instalando a aplica\u00e7\u00e3o"},{"location":"access_data_local/#criando-um-projeto-no-google-cloud","text":"Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico.","title":"Criando um projeto no Google Cloud"},{"location":"access_data_local/#fazendo-queries","text":"Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipios' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"bigrquery\" )) install.packages ( \"bigrquery\" ) library ( \"bigrquery\" ) billing_project_id = \"<YOUR_PROJECT_ID>\" pib_per_capita = \"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" d <- bq_table_download ( bq_project_query ( billing_project_id , pib_per_capita ), page_size = 500 , bigint = \"integer64\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Fazendo queries"},{"location":"analysis/","text":"Se divertindo com a BD+ Todo esse esfor\u00e7o tem uma recompensa: criamos um banco de dados integrado com diversas bases pronto para an\u00e1lises e visualiza\u00e7\u00f5es. Nessa se\u00e7\u00e3o descrevemos coisas que podem ser \u00fateis aos usu\u00e1rios de diversos p\u00fablicos. Tem ideias de an\u00e1lises, gr\u00e1ficos, ou reportagens para escrever em cima do nosso reposit\u00f3rio? Fique a vontade pra come\u00e7ar! S\u00f3 n\u00e3o esque\u00e7a de citar o projeto ;) Como cruzar tabelas no BD+ Podemos cruzar tabelas na BD+ fazendo joins em cima de chaves externas ( foreign keys ). Essas s\u00e3o colunas que servem para identificar unicamente entidades no reposit\u00f3rio. Crit\u00e9rios para uma vari\u00e1vel ser uma chave externa: Nome da vari\u00e1vel \u00e9 \u00fanico entre todos as bases do reposit\u00f3rio BD+; Identifica unicamente a entidade e n\u00e3o tem valores nulos na tabela correspondente de diret\u00f3rio. Chaves geogr\u00e1ficas Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao Chaves temporais ano , semestre , mes , semana , dia , hora Chaves de pessoas f\u00edsicas cpf , pis , nis Chaves de pessoas jur\u00eddicas Empresa: cnpj Escola: id_escola Chaves em pol\u00edtica Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"In\u00edcio"},{"location":"analysis/#se-divertindo-com-a-bd","text":"Todo esse esfor\u00e7o tem uma recompensa: criamos um banco de dados integrado com diversas bases pronto para an\u00e1lises e visualiza\u00e7\u00f5es. Nessa se\u00e7\u00e3o descrevemos coisas que podem ser \u00fateis aos usu\u00e1rios de diversos p\u00fablicos. Tem ideias de an\u00e1lises, gr\u00e1ficos, ou reportagens para escrever em cima do nosso reposit\u00f3rio? Fique a vontade pra come\u00e7ar! S\u00f3 n\u00e3o esque\u00e7a de citar o projeto ;)","title":"Se divertindo com a BD+"},{"location":"analysis/#como-cruzar-tabelas-no-bd","text":"Podemos cruzar tabelas na BD+ fazendo joins em cima de chaves externas ( foreign keys ). Essas s\u00e3o colunas que servem para identificar unicamente entidades no reposit\u00f3rio. Crit\u00e9rios para uma vari\u00e1vel ser uma chave externa: Nome da vari\u00e1vel \u00e9 \u00fanico entre todos as bases do reposit\u00f3rio BD+; Identifica unicamente a entidade e n\u00e3o tem valores nulos na tabela correspondente de diret\u00f3rio.","title":"Como cruzar tabelas no BD+"},{"location":"analysis/#chaves-geograficas","text":"Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao","title":"Chaves geogr\u00e1ficas"},{"location":"analysis/#chaves-temporais","text":"ano , semestre , mes , semana , dia , hora","title":"Chaves temporais"},{"location":"analysis/#chaves-de-pessoas-fisicas","text":"cpf , pis , nis","title":"Chaves de pessoas f\u00edsicas"},{"location":"analysis/#chaves-de-pessoas-juridicas","text":"Empresa: cnpj Escola: id_escola","title":"Chaves de pessoas jur\u00eddicas"},{"location":"analysis/#chaves-em-politica","text":"Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"Chaves em pol\u00edtica"},{"location":"cli_reference_api/","text":"CLI API Esta API \u00e9 composta de comandos para gerenciamento e inser\u00e7\u00e3o de dados no Google Cloud. Os comandos dispon\u00edveis dentro de cli atualmente s\u00e3o: config , que \u00e9 utilizado para atualizar seus dados de configura\u00e7\u00e3o e gerenciar templates dataset , que permite gerenciar datasets no BigQuery (criar, modificar, publicar , atualizar e deletar) download , que permite baixar/salvar queries de tabelas do BigQuery na sua m\u00e1quina local storage , que permite gerenciar seu Storage no BigQuery (criar e subir arquivos) table , que permite gerenciar tabelas no BigQuery (criar, modificar, publicar , atualizar e deletar) Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas cli Usage: cli [OPTIONS] COMMAND [ARGS]... Options: --templates TEXT Templates path --bucket_name TEXT Project bucket name --metadata_path TEXT Folder to store metadata --help Show this message and exit. cli config Usage: cli config [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. cli config overwrite_cli_config Overwrite current configuration Usage: cli config overwrite_cli_config [OPTIONS] Options: --help Show this message and exit. cli config refresh_template Overwrite current templates Usage: cli config refresh_template [OPTIONS] Options: --help Show this message and exit. cli dataset Usage: cli dataset [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. cli dataset create Create dataset on BigQuery Usage: cli dataset create [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --if_exists TEXT [raise|update|replace|pass] if dataset alread exists --help Show this message and exit. cli dataset delete Delete dataset Usage: cli dataset delete [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --help Show this message and exit. cli dataset init Initialize metadata files of dataset Usage: cli dataset init [OPTIONS] DATASET_ID Options: --replace Whether to replace current metadata files --help Show this message and exit. cli dataset publicize Make a dataset public Usage: cli dataset publicize [OPTIONS] DATASET_ID Options: --help Show this message and exit. cli dataset update Update dataset on BigQuery Usage: cli dataset update [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --help Show this message and exit. cli download Download data. You can add extra arguments accepted by pandas.to_csv . Examples: --delimiter='|', --index=False Usage: cli download [OPTIONS] SAVEPATH Options: --dataset_id TEXT Dataset_id, enter with table_id to download table --table_id TEXT Table_id, enter with dataset_id to download table --query TEXT A SQL Standard query to download data from BigQuery --query_project_id TEXT Which project the table lives. You can change this you want to query different projects. --billing_project_id TEXT Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectsele ctor2/home/dashboard --limit TEXT Number of rows returned --help Show this message and exit. cli storage Usage: cli storage [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. cli storage copy_table Copy table to your bucket Usage: cli storage copy_table [OPTIONS] DATASET_ID TABLE_ID Options: --source_bucket_name TEXT [required] --dst_bucket_name TEXT Bucket where data will be copied to, defaults to your bucket -m, --mode TEXT [raw|staging] which bucket folder to get the table --help Show this message and exit. cli storage delete_table Delete table from bucket Usage: cli storage delete_table [OPTIONS] DATASET_ID TABLE_ID Options: -m, --mode TEXT [raw|staging] where to delete the file from [required] --bucket_name TEXT Bucket from which to delete data, you can change it to delete from a bucket other than yours --not_found_ok TEXT what to do if table not found --help Show this message and exit. cli storage init Create bucket and initial folders Usage: cli storage init [OPTIONS] Options: --bucket_name TEXT Bucket name --replace Whether to replace current bucket files --very-sure / --not-sure Are you sure that you want to replace current bucket files? --help Show this message and exit. cli storage upload Upload file to bucket Usage: cli storage upload [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: -m, --mode TEXT [raw|staging] where to save the file [required] --partitions TEXT Data partition as `value=key/value2=key2` --if_exists TEXT [raise|replace|pass] if file alread exists --help Show this message and exit. cli table Usage: cli table [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. cli table append Append new data to existing table Usage: cli table append [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: --partitions TEXT Data partition as `value=key/value2=key2` --if_exists TEXT [raise|replace|pass] if file alread exists --help Show this message and exit. cli table create Create stagging table in BigQuery Usage: cli table create [OPTIONS] DATASET_ID TABLE_ID Options: -p, --path PATH Path of data folder or file. --job_config_params TEXT File to advanced load config params --partitioned [True|False] whether folder has partitions --if_table_exists TEXT [raise|replace|pass] actions if table exists --force_dataset TEXT Whether to automatically create the dataset folders and in BigQuery --if_storage_data_exists TEXT [raise|replace|pass] actions if table data already exists at Storage --if_table_config_exists TEXT [raise|replace|pass] actions if table config files already exist --help Show this message and exit. cli table delete Delete BigQuery table Usage: cli table delete [OPTIONS] DATASET_ID TABLE_ID Options: --mode TEXT Which table to delete [all|prod|staging] [required] --help Show this message and exit. cli table init Create metadata files Usage: cli table init [OPTIONS] DATASET_ID TABLE_ID Options: --data_sample_path PATH Sample data used to pre-fill metadata --if_folder_exists TEXT [raise|replace|pass] actions if table folder exists --if_table_config_exists TEXT [raise|replace|pass] actions if table config files already exist --help Show this message and exit. cli table publish Publish staging table to prod Usage: cli table publish [OPTIONS] DATASET_ID TABLE_ID Options: --if_exists TEXT [raise|replace] actions if table exists --help Show this message and exit. cli table update Update tables in BigQuery Usage: cli table update [OPTIONS] DATASET_ID TABLE_ID Options: --mode TEXT Choose a table from a dataset to update [all|staging|prod] --help Show this message and exit.","title":"CLI"},{"location":"cli_reference_api/#cli-api","text":"Esta API \u00e9 composta de comandos para gerenciamento e inser\u00e7\u00e3o de dados no Google Cloud. Os comandos dispon\u00edveis dentro de cli atualmente s\u00e3o: config , que \u00e9 utilizado para atualizar seus dados de configura\u00e7\u00e3o e gerenciar templates dataset , que permite gerenciar datasets no BigQuery (criar, modificar, publicar , atualizar e deletar) download , que permite baixar/salvar queries de tabelas do BigQuery na sua m\u00e1quina local storage , que permite gerenciar seu Storage no BigQuery (criar e subir arquivos) table , que permite gerenciar tabelas no BigQuery (criar, modificar, publicar , atualizar e deletar) Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"CLI API"},{"location":"cli_reference_api/#cli","text":"Usage: cli [OPTIONS] COMMAND [ARGS]... Options: --templates TEXT Templates path --bucket_name TEXT Project bucket name --metadata_path TEXT Folder to store metadata --help Show this message and exit.","title":"cli"},{"location":"cli_reference_api/#cli-config","text":"Usage: cli config [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"config"},{"location":"cli_reference_api/#cli-config-overwrite_cli_config","text":"Overwrite current configuration Usage: cli config overwrite_cli_config [OPTIONS] Options: --help Show this message and exit.","title":"overwrite_cli_config"},{"location":"cli_reference_api/#cli-config-refresh_template","text":"Overwrite current templates Usage: cli config refresh_template [OPTIONS] Options: --help Show this message and exit.","title":"refresh_template"},{"location":"cli_reference_api/#cli-dataset","text":"Usage: cli dataset [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"dataset"},{"location":"cli_reference_api/#cli-dataset-create","text":"Create dataset on BigQuery Usage: cli dataset create [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --if_exists TEXT [raise|update|replace|pass] if dataset alread exists --help Show this message and exit.","title":"create"},{"location":"cli_reference_api/#cli-dataset-delete","text":"Delete dataset Usage: cli dataset delete [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --help Show this message and exit.","title":"delete"},{"location":"cli_reference_api/#cli-dataset-init","text":"Initialize metadata files of dataset Usage: cli dataset init [OPTIONS] DATASET_ID Options: --replace Whether to replace current metadata files --help Show this message and exit.","title":"init"},{"location":"cli_reference_api/#cli-dataset-publicize","text":"Make a dataset public Usage: cli dataset publicize [OPTIONS] DATASET_ID Options: --help Show this message and exit.","title":"publicize"},{"location":"cli_reference_api/#cli-dataset-update","text":"Update dataset on BigQuery Usage: cli dataset update [OPTIONS] DATASET_ID Options: -m, --mode TEXT What datasets to create [all|staging|prod] --help Show this message and exit.","title":"update"},{"location":"cli_reference_api/#cli-download","text":"Download data. You can add extra arguments accepted by pandas.to_csv . Examples: --delimiter='|', --index=False Usage: cli download [OPTIONS] SAVEPATH Options: --dataset_id TEXT Dataset_id, enter with table_id to download table --table_id TEXT Table_id, enter with dataset_id to download table --query TEXT A SQL Standard query to download data from BigQuery --query_project_id TEXT Which project the table lives. You can change this you want to query different projects. --billing_project_id TEXT Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectsele ctor2/home/dashboard --limit TEXT Number of rows returned --help Show this message and exit.","title":"download"},{"location":"cli_reference_api/#cli-storage","text":"Usage: cli storage [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"storage"},{"location":"cli_reference_api/#cli-storage-copy_table","text":"Copy table to your bucket Usage: cli storage copy_table [OPTIONS] DATASET_ID TABLE_ID Options: --source_bucket_name TEXT [required] --dst_bucket_name TEXT Bucket where data will be copied to, defaults to your bucket -m, --mode TEXT [raw|staging] which bucket folder to get the table --help Show this message and exit.","title":"copy_table"},{"location":"cli_reference_api/#cli-storage-delete_table","text":"Delete table from bucket Usage: cli storage delete_table [OPTIONS] DATASET_ID TABLE_ID Options: -m, --mode TEXT [raw|staging] where to delete the file from [required] --bucket_name TEXT Bucket from which to delete data, you can change it to delete from a bucket other than yours --not_found_ok TEXT what to do if table not found --help Show this message and exit.","title":"delete_table"},{"location":"cli_reference_api/#cli-storage-init","text":"Create bucket and initial folders Usage: cli storage init [OPTIONS] Options: --bucket_name TEXT Bucket name --replace Whether to replace current bucket files --very-sure / --not-sure Are you sure that you want to replace current bucket files? --help Show this message and exit.","title":"init"},{"location":"cli_reference_api/#cli-storage-upload","text":"Upload file to bucket Usage: cli storage upload [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: -m, --mode TEXT [raw|staging] where to save the file [required] --partitions TEXT Data partition as `value=key/value2=key2` --if_exists TEXT [raise|replace|pass] if file alread exists --help Show this message and exit.","title":"upload"},{"location":"cli_reference_api/#cli-table","text":"Usage: cli table [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"table"},{"location":"cli_reference_api/#cli-table-append","text":"Append new data to existing table Usage: cli table append [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: --partitions TEXT Data partition as `value=key/value2=key2` --if_exists TEXT [raise|replace|pass] if file alread exists --help Show this message and exit.","title":"append"},{"location":"cli_reference_api/#cli-table-create","text":"Create stagging table in BigQuery Usage: cli table create [OPTIONS] DATASET_ID TABLE_ID Options: -p, --path PATH Path of data folder or file. --job_config_params TEXT File to advanced load config params --partitioned [True|False] whether folder has partitions --if_table_exists TEXT [raise|replace|pass] actions if table exists --force_dataset TEXT Whether to automatically create the dataset folders and in BigQuery --if_storage_data_exists TEXT [raise|replace|pass] actions if table data already exists at Storage --if_table_config_exists TEXT [raise|replace|pass] actions if table config files already exist --help Show this message and exit.","title":"create"},{"location":"cli_reference_api/#cli-table-delete","text":"Delete BigQuery table Usage: cli table delete [OPTIONS] DATASET_ID TABLE_ID Options: --mode TEXT Which table to delete [all|prod|staging] [required] --help Show this message and exit.","title":"delete"},{"location":"cli_reference_api/#cli-table-init","text":"Create metadata files Usage: cli table init [OPTIONS] DATASET_ID TABLE_ID Options: --data_sample_path PATH Sample data used to pre-fill metadata --if_folder_exists TEXT [raise|replace|pass] actions if table folder exists --if_table_config_exists TEXT [raise|replace|pass] actions if table config files already exist --help Show this message and exit.","title":"init"},{"location":"cli_reference_api/#cli-table-publish","text":"Publish staging table to prod Usage: cli table publish [OPTIONS] DATASET_ID TABLE_ID Options: --if_exists TEXT [raise|replace] actions if table exists --help Show this message and exit.","title":"publish"},{"location":"cli_reference_api/#cli-table-update","text":"Update tables in BigQuery Usage: cli table update [OPTIONS] DATASET_ID TABLE_ID Options: --mode TEXT Choose a table from a dataset to update [all|staging|prod] --help Show this message and exit.","title":"update"},{"location":"colab_data/","text":"Colaborando com dados na BD+ Para criar um reposit\u00f3rio integrado de dados p\u00fablicos precisamos subir bases de dados de centenas de fontes e tamb\u00e9m mant\u00ea-las consistentes entre si, atualizadas, e \u00fateis . Nossa inten\u00e7\u00e3o \u00e9 que qualquer pessoa possa colaborar com seu conhecimento sobre dados p\u00fablicos, seguindo nossas metodologias e procedimentos de limpeza e valida\u00e7\u00e3o dos dados. Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) Qual o procedimento? Adicionar bases novas na BD+ deve seguir nosso fluxo de trabalho: Informe seu interesse para a gente . Limpe e trate dados . Suba dados no BigQuery Envie c\u00f3digo e dados prontos para revis\u00e3o. 1. Informe seu interesse para a gente Mantemos metadados de bases que ainda n\u00e3o est\u00e3o na BD+ em nossa tabela de prioridade de bases . Para subir uma base de seu interesse na BD+, adicione os seus metadados acordo com as colunas da tabela. Inclua uma linha por tabela do conjunto de dados. Preencha os campos cuidadosamente . Al\u00e9m dos dados gerais sobre a tabela, adicione as classifica\u00e7\u00f5es de Facilidade e Import\u00e2ncia e o campo de Prioridade ser\u00e1 gerado automaticamente. Por fim, crie um issue no Github com o template \"Novos dados\" Caso sua base j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github na coluna Pessoa respons\u00e1vel . 2. Limpe e trate os dados Estrutura de pastas Sugerimos a seguinte estrutura local da pasta que voc\u00ea ir\u00e1 trabalhar num determinado conjunto de dados: nome_conjunto (um conjunto pode ter mais de uma tabela aqui) /code Pasta onde ficam todos os scripts (c\u00f3digos) necess\u00e1rios \u00e0 limpeza dos dados. Nessa configura\u00e7\u00e3o, toda a estrutura de c\u00f3digo ser\u00e1 com atalhos relativos \u00e0 pasta ra\u00edz, usando as demais pastas criadas. /input Pasta onde ficam todos os arquivos com dados originais, como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem jamais ser modificados. /output Pasta para arquivos finais, j\u00e1 no formato e conte\u00fado prontos para subir na BD+. /tmp Pasta usada para quaisquer arquivos tempor\u00e1rios criados pelos c\u00f3digos em /code no processo de limpeza e tratamento. Captura dos dados De in\u00edcio, baixe os dados originais e os organize na pasta /input . Idealmente esse processo deve ser automatizado por um script espec\u00edfico que possa ser reutilizado, mas isso n\u00e3o \u00e9 obrigat\u00f3rio. Arquitetura dos dados \u00c9 muito importante pensar a arquitetura da base antes de se come\u00e7ar a limpeza . Perguntas que uma arquitetura deve responder: Quais ser\u00e3o as tabelas finais na base? Essas n\u00e3o precisam ser exatamente o que veio nos dados brutos. Qual \u00e9 n\u00edvel da observa\u00e7\u00e3o de cada tabela? O \"n\u00edvel da observa\u00e7\u00e3o\" \u00e9 a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato, estabelecimento-cnae). Ser\u00e1 gerada alguma tabela derivada com agrega\u00e7\u00f5es em cima dos dados originais? Limpeza dos dados A limpeza de dados \u00e9 parte fundamental da nossa estrutura. O c\u00f3digo deve seguir boas pr\u00e1ticas de programa\u00e7\u00e3o na linguagem de sua prefer\u00eancia, e os dados devem ser tratados garantindo que: As colunas seguem as diretrizes Base dos Dados Os valores seguem as diretrizes Base dos Dados Sugerimos as linguagens Python , R , ou Stata . 3. Suba dados no BigQuery Desenvolvemos um cliente basedosdados (dispon\u00edvel para linha de comando e Python por enquanto) para facilitar esse processo e indicar configura\u00e7\u00f5es b\u00e1sicas que devem ser preenchidas sobre os dados. Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Bucket (Storage): local onde ser\u00e3o armazenados o arquivos frios Big Query: banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging: banco para teste e tratamento final do conjunto de dados Prod: banco oficial de publica\u00e7\u00e3o dos dados ( basedosdados ! ou o seu mesmo caso queira reproduzir o ambiente) Configure seu projeto no Google Cloud e um bucket no Google Storage Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. Basta seguir o passo-a-passo: Acesse o link e aceite o Termo de Servi\u00e7os do Google Cloud. Clique em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta, seu bucket , para voc\u00ea subir os dados. Por fim, no seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Googl Cloud. Suba e configure uma tabela no seu bucket Siga o comando basedosdados table create [DATASET_ID] [TABLE_ID] . Preencha todos os arquivos de configura\u00e7\u00e3o: README.md : informa\u00e7\u00f5es b\u00e1sicas da base de dados aparecendo no Github dataset_config.yaml : informa\u00e7\u00f5es espec\u00edficas da base de dados /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela /[TABLE_ID]/publish.sql : informa\u00e7\u00f5es da publica\u00e7\u00e3o da tabela, como os tipos de vari\u00e1veis do BigQuery ( STRING , INT64 , FLOAT64 , DATE ), e use tamb\u00e9m este espa\u00e7o para escrever tratamentos finais na tabela staging em SQL para publicacao Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo. Publique a vers\u00e3o pronta no seu bucket com basedosdados table publish Ap\u00f3s rodar o publish , v\u00e1 no BigQuery e verifique se os dados subiram rodando SELECT * FROM [PROJECT_ID].[DATASET_ID].[TABLE_ID] . 4. Envie c\u00f3digo e dados prontos para revis\u00e3o Basta seguir o passo a passo do comando basedosdados table release . Isto ir\u00e1 criar um pull request no nosso Github para validarmos. As etapas de valida\u00e7\u00e3o que seguimos (devem ser feitas por outra pessoa al\u00e9m de quem subiu os dados) s\u00e3o: Testar a query sem limite de observa\u00e7\u00f5es; Revisar todos os campos dos arquivos de configura\u00e7\u00e3o; Checar nomea\u00e7\u00e3o de vari\u00e1veis e formatos e unidades de dados de acordo com nosso manual de estilo.","title":"Dados"},{"location":"colab_data/#colaborando-com-dados-na-bd","text":"Para criar um reposit\u00f3rio integrado de dados p\u00fablicos precisamos subir bases de dados de centenas de fontes e tamb\u00e9m mant\u00ea-las consistentes entre si, atualizadas, e \u00fateis . Nossa inten\u00e7\u00e3o \u00e9 que qualquer pessoa possa colaborar com seu conhecimento sobre dados p\u00fablicos, seguindo nossas metodologias e procedimentos de limpeza e valida\u00e7\u00e3o dos dados. Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Colaborando com dados na BD+"},{"location":"colab_data/#qual-o-procedimento","text":"Adicionar bases novas na BD+ deve seguir nosso fluxo de trabalho: Informe seu interesse para a gente . Limpe e trate dados . Suba dados no BigQuery Envie c\u00f3digo e dados prontos para revis\u00e3o.","title":"Qual o procedimento?"},{"location":"colab_data/#1-informe-seu-interesse-para-a-gente","text":"Mantemos metadados de bases que ainda n\u00e3o est\u00e3o na BD+ em nossa tabela de prioridade de bases . Para subir uma base de seu interesse na BD+, adicione os seus metadados acordo com as colunas da tabela. Inclua uma linha por tabela do conjunto de dados. Preencha os campos cuidadosamente . Al\u00e9m dos dados gerais sobre a tabela, adicione as classifica\u00e7\u00f5es de Facilidade e Import\u00e2ncia e o campo de Prioridade ser\u00e1 gerado automaticamente. Por fim, crie um issue no Github com o template \"Novos dados\" Caso sua base j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github na coluna Pessoa respons\u00e1vel .","title":"1. Informe seu interesse para a gente"},{"location":"colab_data/#2-limpe-e-trate-os-dados","text":"","title":"2. Limpe e trate os dados"},{"location":"colab_data/#estrutura-de-pastas","text":"Sugerimos a seguinte estrutura local da pasta que voc\u00ea ir\u00e1 trabalhar num determinado conjunto de dados: nome_conjunto (um conjunto pode ter mais de uma tabela aqui) /code Pasta onde ficam todos os scripts (c\u00f3digos) necess\u00e1rios \u00e0 limpeza dos dados. Nessa configura\u00e7\u00e3o, toda a estrutura de c\u00f3digo ser\u00e1 com atalhos relativos \u00e0 pasta ra\u00edz, usando as demais pastas criadas. /input Pasta onde ficam todos os arquivos com dados originais, como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem jamais ser modificados. /output Pasta para arquivos finais, j\u00e1 no formato e conte\u00fado prontos para subir na BD+. /tmp Pasta usada para quaisquer arquivos tempor\u00e1rios criados pelos c\u00f3digos em /code no processo de limpeza e tratamento.","title":"Estrutura de pastas"},{"location":"colab_data/#captura-dos-dados","text":"De in\u00edcio, baixe os dados originais e os organize na pasta /input . Idealmente esse processo deve ser automatizado por um script espec\u00edfico que possa ser reutilizado, mas isso n\u00e3o \u00e9 obrigat\u00f3rio.","title":"Captura dos dados"},{"location":"colab_data/#arquitetura-dos-dados","text":"\u00c9 muito importante pensar a arquitetura da base antes de se come\u00e7ar a limpeza . Perguntas que uma arquitetura deve responder: Quais ser\u00e3o as tabelas finais na base? Essas n\u00e3o precisam ser exatamente o que veio nos dados brutos. Qual \u00e9 n\u00edvel da observa\u00e7\u00e3o de cada tabela? O \"n\u00edvel da observa\u00e7\u00e3o\" \u00e9 a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato, estabelecimento-cnae). Ser\u00e1 gerada alguma tabela derivada com agrega\u00e7\u00f5es em cima dos dados originais?","title":"Arquitetura dos dados"},{"location":"colab_data/#limpeza-dos-dados","text":"A limpeza de dados \u00e9 parte fundamental da nossa estrutura. O c\u00f3digo deve seguir boas pr\u00e1ticas de programa\u00e7\u00e3o na linguagem de sua prefer\u00eancia, e os dados devem ser tratados garantindo que: As colunas seguem as diretrizes Base dos Dados Os valores seguem as diretrizes Base dos Dados Sugerimos as linguagens Python , R , ou Stata .","title":"Limpeza dos dados"},{"location":"colab_data/#3-suba-dados-no-bigquery","text":"Desenvolvemos um cliente basedosdados (dispon\u00edvel para linha de comando e Python por enquanto) para facilitar esse processo e indicar configura\u00e7\u00f5es b\u00e1sicas que devem ser preenchidas sobre os dados. Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Bucket (Storage): local onde ser\u00e3o armazenados o arquivos frios Big Query: banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging: banco para teste e tratamento final do conjunto de dados Prod: banco oficial de publica\u00e7\u00e3o dos dados ( basedosdados ! ou o seu mesmo caso queira reproduzir o ambiente)","title":"3. Suba dados no BigQuery"},{"location":"colab_data/#configure-seu-projeto-no-google-cloud-e-um-bucket-no-google-storage","text":"Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. Basta seguir o passo-a-passo: Acesse o link e aceite o Termo de Servi\u00e7os do Google Cloud. Clique em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta, seu bucket , para voc\u00ea subir os dados. Por fim, no seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Googl Cloud.","title":"Configure seu projeto no Google Cloud e um bucket no Google Storage"},{"location":"colab_data/#suba-e-configure-uma-tabela-no-seu-bucket","text":"Siga o comando basedosdados table create [DATASET_ID] [TABLE_ID] . Preencha todos os arquivos de configura\u00e7\u00e3o: README.md : informa\u00e7\u00f5es b\u00e1sicas da base de dados aparecendo no Github dataset_config.yaml : informa\u00e7\u00f5es espec\u00edficas da base de dados /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela /[TABLE_ID]/publish.sql : informa\u00e7\u00f5es da publica\u00e7\u00e3o da tabela, como os tipos de vari\u00e1veis do BigQuery ( STRING , INT64 , FLOAT64 , DATE ), e use tamb\u00e9m este espa\u00e7o para escrever tratamentos finais na tabela staging em SQL para publicacao Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo.","title":"Suba e configure uma tabela no seu bucket"},{"location":"colab_data/#publique-a-versao-pronta-no-seu-bucket-com-basedosdados-table-publish","text":"Ap\u00f3s rodar o publish , v\u00e1 no BigQuery e verifique se os dados subiram rodando SELECT * FROM [PROJECT_ID].[DATASET_ID].[TABLE_ID] .","title":"Publique a vers\u00e3o pronta no seu bucket com basedosdados table publish"},{"location":"colab_data/#4-envie-codigo-e-dados-prontos-para-revisao","text":"Basta seguir o passo a passo do comando basedosdados table release . Isto ir\u00e1 criar um pull request no nosso Github para validarmos. As etapas de valida\u00e7\u00e3o que seguimos (devem ser feitas por outra pessoa al\u00e9m de quem subiu os dados) s\u00e3o: Testar a query sem limite de observa\u00e7\u00f5es; Revisar todos os campos dos arquivos de configura\u00e7\u00e3o; Checar nomea\u00e7\u00e3o de vari\u00e1veis e formatos e unidades de dados de acordo com nosso manual de estilo.","title":"4. Envie c\u00f3digo e dados prontos para revis\u00e3o"},{"location":"py_reference_api/","text":"Python API Esta API \u00e9 composta de m\u00f3dulos para requisi\u00e7\u00e3o de dados : : download , que permite baixar tabelas do BigQuery em CSV direto na sua m\u00e1quina. read_sql , que permite fazer uma query SQL e carregar os dados no ambiente do Python. read_table , que permite ler uma tabela do BigQuery pelo nome e carregar os dados no ambiente do Python. E tamb\u00e9m de classes para gerenciamento de dados no Google Cloud: Storage , que permite manusear arquivos no Storage Dataset , que permite manusear datasets no BigQuery Table , que permite manusear tables Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas M\u00f3dulos download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = 'basedosdados' , billing_project_id = None , limit = None , reauth = False , ** pandas_kwargs ) Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath If savepath is a folder, it saves a file as savepath / table_id.csv or savepath / query_result.csv if table_id not available. If savepath is a file, saves data to file. required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None limit int Optional Number of rows. None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False pandas_kwargs Extra arguments accepted by pandas.to_csv {} Exceptions: Type Description Exception If either table_id or dataset_id were are empty. Source code in basedosdados/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , reauth = False , ** pandas_kwargs , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): If savepath is a folder, it saves a file as `savepath / table_id.csv` or `savepath / query_result.csv` if table_id not available. If savepath is a file, saves data to file. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard limit (int): Optional Number of rows. reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. pandas_kwargs (): Extra arguments accepted by [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) Raises: Exception: If either table_id or dataset_id were are empty. \"\"\" savepath = Path ( savepath ) # make sure that path exists if savepath . is_dir (): savepath . mkdir ( parents = True , exist_ok = True ) else : savepath . parent . mkdir ( parents = True , exist_ok = True ) if ( dataset_id is not None ) and ( table_id is not None ): table = read_table ( dataset_id , table_id , query_project_id = query_project_id , billing_project_id = billing_project_id , limit = limit , reauth = reauth , ) elif query is not None : query += f \" limit { limit } \" if limit is not None else \"\" table = read_sql ( query , billing_project_id = billing_project_id , reauth = reauth ) elif query is None : raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) if savepath . is_dir (): if table_id is not None : savepath = savepath / ( table_id + \".csv\" ) else : savepath = savepath / ( \"query_result.csv\" ) table . to_csv ( savepath , ** pandas_kwargs ) get_dataset_description ( dataset_id = None , query_project_id = 'basedosdados' ) Prints the full dataset description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Returns: Type Description None. Source code in basedosdados/download.py def get_dataset_description ( dataset_id = None , query_project_id = \"basedosdados\" ): \"\"\"Prints the full dataset description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) print ( dataset . description ) return None get_table_columns ( dataset_id = None , table_id = None , query_project_id = 'basedosdados' ) Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Examples: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Returns: Type Description None. Source code in basedosdados/download.py def get_table_columns ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , ): \"\"\"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Example: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) table_ref = client . get_table ( f \" { dataset_id } . { table_id } \" ) columns = [ ( field . name , field . field_type , field . description ) for field in table_ref . schema ] description = pd . DataFrame ( columns , columns = [ \"name\" , \"field_type\" , \"description\" ]) _print_output ( description ) return None get_table_description ( dataset_id = None , table_id = None , query_project_id = 'basedosdados' ) Prints the full table description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Returns: Type Description None. Source code in basedosdados/download.py def get_table_description ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" ): \"\"\"Prints the full table description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) table = client . get_table ( f \" { dataset_id } . { table_id } \" ) print ( table . description ) return None get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = 'basedosdados' ) Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard required Examples: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Returns: Type Description None Source code in basedosdados/download.py def get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = \"basedosdados\" , ): \"\"\"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard Example: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Returns: None \"\"\" billing_client = bigquery . Client ( credentials = credentials (), project = billing_project_id ) query = f \"\"\"SELECT COUNT(*) FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" job = billing_client . query ( query , location = \"US\" ) num_rows = job . to_dataframe () . loc [ 0 , \"f0_\" ] size_mb = round ( job . total_bytes_processed / 1024 / 1024 , 2 ) table_data = pd . DataFrame ( [ { \"project_id\" : query_project_id , \"dataset_id\" : dataset_id , \"table_id\" : table_id , \"num_rows\" : num_rows , \"size_mb\" : size_mb , } ] ) _print_output ( table_data ) return None list_dataset_tables ( dataset_id , query_project_id = 'basedosdados' , filter_by = None , with_description = False ) Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in the table_id. None with_description bool Optional If True, fetch short table descriptions for each table that match the search criteria. False Examples: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Returns: Type Description None. Source code in basedosdados/download.py def list_dataset_tables ( dataset_id , query_project_id = \"basedosdados\" , filter_by = None , with_description = False , ): \"\"\"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in the table_id. with_description (bool): Optional If True, fetch short table descriptions for each table that match the search criteria. Example: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) tables_list = list ( client . list_tables ( dataset )) tables = pd . DataFrame ( [ table . table_id for table in tables_list ], columns = [ \"table_id\" ] ) if filter_by : tables = tables . loc [ tables [ \"table_id\" ] . str . contains ( filter_by )] if with_description : tables [ \"description\" ] = [ _get_header ( client . get_table ( f \" { dataset_id } . { table } \" ) . description ) for table in tables [ \"table_id\" ] ] _print_output ( tables ) return None list_datasets ( query_project_id = 'basedosdados' , filter_by = None , with_description = False ) Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Parameters: Name Type Description Default query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in dataset_id. None with_description bool Optional If True, fetch short dataset description for each dataset. False Examples: list_datasets( filter_by='sp', with_description=True, ) Returns: Type Description None. Source code in basedosdados/download.py def list_datasets ( query_project_id = \"basedosdados\" , filter_by = None , with_description = False , ): \"\"\"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Args: query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in dataset_id. with_description (bool): Optional If True, fetch short dataset description for each dataset. Example: list_datasets( filter_by='sp', with_description=True, ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) datasets_list = list ( client . list_datasets ()) datasets = pd . DataFrame ( [ dataset . dataset_id for dataset in datasets_list ], columns = [ \"dataset_id\" ] ) if filter_by : datasets = datasets . loc [ datasets [ \"dataset_id\" ] . str . contains ( filter_by )] if with_description : datasets [ \"description\" ] = [ _get_header ( client . get_dataset ( dataset ) . description ) for dataset in datasets [ \"dataset_id\" ] ] _print_output ( datasets ) return None read_sql ( query , billing_project_id = None , reauth = False ) Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download.py def read_sql ( query , billing_project_id = None , reauth = False ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. Returns: pd.DataFrame: Query result \"\"\" try : return pandas_gbq . read_gbq ( query , credentials = credentials ( reauth = reauth ), project_id = billing_project_id , ) except ( OSError , ValueError ): raise BaseDosDadosException ( \" \\n We are not sure which Google Cloud project should be billed. \\n \" \"First, you should make sure that you have a Google Cloud project. \\n \" \"If you don't have one, set one up following these steps: \\n \" \" \\t 1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \" \\t 2. Agree with Terms of Service if asked \\n \" \" \\t 3. Click in Create Project \\n \" \" \\t 4. Put a cool name in your project \\n \" \" \\t 5. Hit create \\n \" \"\" \"Copy the Project ID, (notice that it is not the Project Name) \\n \" \"Now, you have two options: \\n \" \"1. Add an argument to your function poiting to the billing project id. \\n \" \" Like `bd.read_table('br_ibge_pib', 'municipios', billing_project_id=<YOUR_PROJECT_ID>)` \\n \" \"2. You can set a project_id in the environment by running the following command in your terminal: `gcloud config set project <YOUR_PROJECT_ID>`.\" \" Bear in mind that you need `gcloud` installed.\" ) except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosException ( \" \\n You still don't have a Google Cloud Project. \\n \" \"Set one up following these steps: \\n \" \"1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \"2. Agree with Terms of Service if asked \\n \" \"3. Click in Create Project \\n \" \"4. Put a cool name in your project \\n \" \"5. Hit create \\n \" \"6. Rerun this command with the flag `reauth=True`. \\n \" \" Like `read_table('br_ibge_pib', 'municipios', reauth=True)`\" ) else : raise e read_table ( dataset_id , table_id , query_project_id = 'basedosdados' , billing_project_id = None , limit = None , reauth = False ) Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False limit int Optional. Number of rows to read from table. None Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download.py def read_table ( dataset_id , table_id , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , reauth = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. limit (int): Optional. Number of rows to read from table. Returns: pd.DataFrame: Query result \"\"\" if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , reauth = reauth ) Classes Storage Manage files on Google Cloud Storage. copy_table ( self , source_bucket_name = 'basedosdados' , destination_bucket_name = None , mode = 'staging' ) Copies table from a source bucket to your bucket, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. TODO: auto divides into multiple requests Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name(str) Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) required mode str Optional Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. #TODO: auto divides into multiple requests Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name(str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Optional Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } \" ) ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) with self . client [ \"storage_staging\" ] . batch (): for blob in source_table_ref : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket ) delete_file ( self , filename , mode , partitions = None , not_found_ok = False ) Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"raw\" , \"staging\" ] else : mode = [ mode ] for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists (): blob . delete () elif not_found_ok : return else : blob . delete () delete_table ( self , mode = 'staging' , bucket_name = None , not_found_ok = False ) Deletes a table from storage, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. TODO: auto divides into multiple requests Parameters: Name Type Description Default mode str Optional Folder of which dataset to update. 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. #TODO: auto divides into multiple requests Args: mode (str): Optional Folder of which dataset to update. bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = self . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) else : table_blobs = self . bucket . list_blobs ( prefix = prefix ) with self . client [ \"storage_staging\" ] . batch (): for blob in table_blobs : self . delete_file ( filename = str ( blob . name ) . replace ( prefix , \"\" ), mode = mode , not_found_ok = not_found_ok , ) init ( self , replace = False , very_sure = False ) Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) else : self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" ) upload ( self , path , mode = 'all' , partitions = None , if_exists = 'raise' , ** upload_args ) Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 3 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 3 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"raw\" , \"staging\" ] else : mode = [ mode ] for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise Exception ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"Set if_exists to 'replace' to overwrite data\" ) Dataset Manage datasets in BigQuery. create ( self , mode = 'all' , if_exists = 'raise' ) Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflic exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflic exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ]) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : job = m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. except Conflict : if if_exists == \"pass\" : return else : raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) # Make prod dataset public self . publicize () delete ( self , mode = 'all' ) Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) init ( self , replace = False ) Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) for file in ( Path ( self . templates ) / \"dataset\" ) . glob ( \"*\" ): if file . name in [ \"dataset_config.yaml\" , \"README.md\" ]: # Load and fill template template = self . _render_template ( f \"dataset/ { file . name } \" , dict ( dataset_id = self . dataset_id ) ) # Write file ( self . dataset_folder / file . name ) . open ( \"w\" ) . write ( template ) # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self publicize ( self , mode = 'all' ) Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' Source code in basedosdados/dataset.py def publicize ( self , mode = \"all\" ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) update ( self , mode = 'all' ) Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' Source code in basedosdados/dataset.py def update ( self , mode = \"all\" ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. dataset = m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ]), fields = [ \"description\" ] ) # Make an API request. Table Manage tables in Google Cloud Storage and BigQuery. append ( self , filepath , partitions = None , if_exists = 'raise' , ** upload_args ) Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Source code in basedosdados/table.py def append ( self , filepath , partitions = None , if_exists = \"raise\" , ** upload_args ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing \"\"\" Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = None , if_exists = if_exists , ** upload_args , ) self . create ( if_exists = \"replace\" ) create ( self , path = None , job_config_params = None , partitioned = False , force_dataset = True , if_table_exists = 'raise' , if_storage_data_exists = 'raise' , if_table_config_exists = 'raise' ) Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> , for instance, year=2012/country=BR Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery None partitioned bool Optional. Whether data is partitioned False if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Implement if_table_exists=raise * Implement if_table_exists=pass Source code in basedosdados/table.py def create ( self , path = None , job_config_params = None , partitioned = False , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>`, for instance, `year=2012/country=BR` Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery partitioned (bool): Optional. Whether data is partitioned if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Implement if_table_exists=raise * Implement if_table_exists=pass \"\"\" # Add data to storage if isinstance ( path , ( str , PosixPath , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , ) external_config = external_config = bigquery . ExternalConfig ( \"CSV\" ) external_config . options . skip_leading_rows = 1 external_config . options . allow_quoted_newlines = True external_config . options . allow_jagged_rows = True external_config . autodetect = False external_config . schema = self . _load_schema ( \"staging\" ) external_config . source_uris = ( f \"gs:// { self . bucket_name } /staging/ { self . dataset_id } / { self . table_id } /*\" ) if partitioned : hive_partitioning = bigquery . external_config . HivePartitioningOptions () hive_partitioning . mode = \"AUTO\" hive_partitioning . source_uri_prefix = self . uri . format ( dataset = self . dataset_id , table = self . table_id ) . replace ( \"*\" , \"\" ) external_config . hive_partitioning = hive_partitioning table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = external_config table_ref = self . client [ \"bigquery_staging\" ] . list_tables ( f \" { self . dataset_id } _staging\" ) table_names = [ table . table_id for table in table_ref ] if self . table_id in table_names : if if_table_exists == \"pass\" : return None elif if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) delete ( self , mode ) Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging|all] required Source code in basedosdados/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging|all] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) init ( self , data_sample_path = None , if_folder_exists = 'raise' , if_table_config_exists = 'raise' ) Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing required Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) elif if_folder_exists == \"pass\" : return self partition_columns = [] if isinstance ( data_sample_path , ( str , PosixPath , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] if data_sample_path . suffix == \".csv\" : columns = next ( csv . reader ( open ( data_sample_path , \"r\" ))) else : raise NotImplementedError ( \"Data sample just supports comma separated csv files\" ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass else : raise FileExistsError ( f \"No config files found at { self . table_folder } \" ) else : if if_table_config_exists == \"raise\" : if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) else : self . _make_template ( columns , partition_columns ) if if_table_config_exists == \"replace\" : self . _make_template ( columns , partition_columns ) return self publish ( self , if_exists = 'raise' ) Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" ) . read () ) self . update ( \"prod\" ) update ( self , mode = 'all' , not_found_ok = True ) Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found True Source code in basedosdados/table.py def update ( self , mode = \"all\" , not_found_ok = True ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"prod\" , \"staging\" ] else : mode = [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue table . description = self . _render_template ( \"table/table_description.txt\" , self . table_config ) # save table description open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , ) . write ( table . description ) # if m == \"prod\":/ table . schema = self . _load_schema ( m ) self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = [ \"description\" , \"schema\" ] )","title":"Python"},{"location":"py_reference_api/#python-api","text":"Esta API \u00e9 composta de m\u00f3dulos para requisi\u00e7\u00e3o de dados : : download , que permite baixar tabelas do BigQuery em CSV direto na sua m\u00e1quina. read_sql , que permite fazer uma query SQL e carregar os dados no ambiente do Python. read_table , que permite ler uma tabela do BigQuery pelo nome e carregar os dados no ambiente do Python. E tamb\u00e9m de classes para gerenciamento de dados no Google Cloud: Storage , que permite manusear arquivos no Storage Dataset , que permite manusear datasets no BigQuery Table , que permite manusear tables Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"Python API"},{"location":"py_reference_api/#modulos","text":"","title":"M\u00f3dulos"},{"location":"py_reference_api/#basedosdados.download","text":"","title":"basedosdados.download"},{"location":"py_reference_api/#basedosdados.download.download","text":"Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath If savepath is a folder, it saves a file as savepath / table_id.csv or savepath / query_result.csv if table_id not available. If savepath is a file, saves data to file. required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None limit int Optional Number of rows. None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False pandas_kwargs Extra arguments accepted by pandas.to_csv {} Exceptions: Type Description Exception If either table_id or dataset_id were are empty. Source code in basedosdados/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , reauth = False , ** pandas_kwargs , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): If savepath is a folder, it saves a file as `savepath / table_id.csv` or `savepath / query_result.csv` if table_id not available. If savepath is a file, saves data to file. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard limit (int): Optional Number of rows. reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. pandas_kwargs (): Extra arguments accepted by [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) Raises: Exception: If either table_id or dataset_id were are empty. \"\"\" savepath = Path ( savepath ) # make sure that path exists if savepath . is_dir (): savepath . mkdir ( parents = True , exist_ok = True ) else : savepath . parent . mkdir ( parents = True , exist_ok = True ) if ( dataset_id is not None ) and ( table_id is not None ): table = read_table ( dataset_id , table_id , query_project_id = query_project_id , billing_project_id = billing_project_id , limit = limit , reauth = reauth , ) elif query is not None : query += f \" limit { limit } \" if limit is not None else \"\" table = read_sql ( query , billing_project_id = billing_project_id , reauth = reauth ) elif query is None : raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) if savepath . is_dir (): if table_id is not None : savepath = savepath / ( table_id + \".csv\" ) else : savepath = savepath / ( \"query_result.csv\" ) table . to_csv ( savepath , ** pandas_kwargs )","title":"download()"},{"location":"py_reference_api/#basedosdados.download.get_dataset_description","text":"Prints the full dataset description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Returns: Type Description None. Source code in basedosdados/download.py def get_dataset_description ( dataset_id = None , query_project_id = \"basedosdados\" ): \"\"\"Prints the full dataset description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) print ( dataset . description ) return None","title":"get_dataset_description()"},{"location":"py_reference_api/#basedosdados.download.get_table_columns","text":"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Examples: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Returns: Type Description None. Source code in basedosdados/download.py def get_table_columns ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" , ): \"\"\"Fetch the names, types and descriptions for the columns in the specified table. Prints information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Example: get_table_columns( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario' ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) table_ref = client . get_table ( f \" { dataset_id } . { table_id } \" ) columns = [ ( field . name , field . field_type , field . description ) for field in table_ref . schema ] description = pd . DataFrame ( columns , columns = [ \"name\" , \"field_type\" , \"description\" ]) _print_output ( description ) return None","title":"get_table_columns()"},{"location":"py_reference_api/#basedosdados.download.get_table_description","text":"Prints the full table description. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' Returns: Type Description None. Source code in basedosdados/download.py def get_table_description ( dataset_id = None , table_id = None , query_project_id = \"basedosdados\" ): \"\"\"Prints the full table description. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) table = client . get_table ( f \" { dataset_id } . { table_id } \" ) print ( table . description ) return None","title":"get_table_description()"},{"location":"py_reference_api/#basedosdados.download.get_table_size","text":"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard required Examples: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Returns: Type Description None Source code in basedosdados/download.py def get_table_size ( dataset_id , table_id , billing_project_id , query_project_id = \"basedosdados\" , ): \"\"\"Use a query to get the number of rows and size (in Mb) of a table query from BigQuery. Prints information on screen in markdown friendly format. WARNING: this query may cost a lot depending on the table. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard Example: get_table_size( dataset_id='br_ibge_censo2010', table_id='pessoa_renda_setor_censitario', billing_project_id='yourprojectid' ) Returns: None \"\"\" billing_client = bigquery . Client ( credentials = credentials (), project = billing_project_id ) query = f \"\"\"SELECT COUNT(*) FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" job = billing_client . query ( query , location = \"US\" ) num_rows = job . to_dataframe () . loc [ 0 , \"f0_\" ] size_mb = round ( job . total_bytes_processed / 1024 / 1024 , 2 ) table_data = pd . DataFrame ( [ { \"project_id\" : query_project_id , \"dataset_id\" : dataset_id , \"table_id\" : table_id , \"num_rows\" : num_rows , \"size_mb\" : size_mb , } ] ) _print_output ( table_data ) return None","title":"get_table_size()"},{"location":"py_reference_api/#basedosdados.download.list_dataset_tables","text":"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in the table_id. None with_description bool Optional If True, fetch short table descriptions for each table that match the search criteria. False Examples: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Returns: Type Description None. Source code in basedosdados/download.py def list_dataset_tables ( dataset_id , query_project_id = \"basedosdados\" , filter_by = None , with_description = False , ): \"\"\"Fetch table_id for tables available at the specified dataset_id. Prints the information on screen. Args: dataset_id (str): Optional. Dataset id available in basedosdados. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in the table_id. with_description (bool): Optional If True, fetch short table descriptions for each table that match the search criteria. Example: list_dataset_tables( dataset_id='br_ibge_censo2010' filter_by='renda', with_description=True, ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) dataset = client . get_dataset ( dataset_id ) tables_list = list ( client . list_tables ( dataset )) tables = pd . DataFrame ( [ table . table_id for table in tables_list ], columns = [ \"table_id\" ] ) if filter_by : tables = tables . loc [ tables [ \"table_id\" ] . str . contains ( filter_by )] if with_description : tables [ \"description\" ] = [ _get_header ( client . get_table ( f \" { dataset_id } . { table } \" ) . description ) for table in tables [ \"table_id\" ] ] _print_output ( tables ) return None","title":"list_dataset_tables()"},{"location":"py_reference_api/#basedosdados.download.list_datasets","text":"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Parameters: Name Type Description Default query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' filter_by str Optional String to be matched in dataset_id. None with_description bool Optional If True, fetch short dataset description for each dataset. False Examples: list_datasets( filter_by='sp', with_description=True, ) Returns: Type Description None. Source code in basedosdados/download.py def list_datasets ( query_project_id = \"basedosdados\" , filter_by = None , with_description = False , ): \"\"\"Fetch the dataset_id of datasets available at query_project_id. Prints information on screen. Args: query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. filter_by (str): Optional String to be matched in dataset_id. with_description (bool): Optional If True, fetch short dataset description for each dataset. Example: list_datasets( filter_by='sp', with_description=True, ) Returns: None. \"\"\" client = bigquery . Client ( credentials = credentials (), project = query_project_id ) datasets_list = list ( client . list_datasets ()) datasets = pd . DataFrame ( [ dataset . dataset_id for dataset in datasets_list ], columns = [ \"dataset_id\" ] ) if filter_by : datasets = datasets . loc [ datasets [ \"dataset_id\" ] . str . contains ( filter_by )] if with_description : datasets [ \"description\" ] = [ _get_header ( client . get_dataset ( dataset ) . description ) for dataset in datasets [ \"dataset_id\" ] ] _print_output ( datasets ) return None","title":"list_datasets()"},{"location":"py_reference_api/#basedosdados.download.read_sql","text":"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download.py def read_sql ( query , billing_project_id = None , reauth = False ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. Returns: pd.DataFrame: Query result \"\"\" try : return pandas_gbq . read_gbq ( query , credentials = credentials ( reauth = reauth ), project_id = billing_project_id , ) except ( OSError , ValueError ): raise BaseDosDadosException ( \" \\n We are not sure which Google Cloud project should be billed. \\n \" \"First, you should make sure that you have a Google Cloud project. \\n \" \"If you don't have one, set one up following these steps: \\n \" \" \\t 1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \" \\t 2. Agree with Terms of Service if asked \\n \" \" \\t 3. Click in Create Project \\n \" \" \\t 4. Put a cool name in your project \\n \" \" \\t 5. Hit create \\n \" \"\" \"Copy the Project ID, (notice that it is not the Project Name) \\n \" \"Now, you have two options: \\n \" \"1. Add an argument to your function poiting to the billing project id. \\n \" \" Like `bd.read_table('br_ibge_pib', 'municipios', billing_project_id=<YOUR_PROJECT_ID>)` \\n \" \"2. You can set a project_id in the environment by running the following command in your terminal: `gcloud config set project <YOUR_PROJECT_ID>`.\" \" Bear in mind that you need `gcloud` installed.\" ) except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosException ( \" \\n You still don't have a Google Cloud Project. \\n \" \"Set one up following these steps: \\n \" \"1. Go to this link https://console.cloud.google.com/projectselector2/home/dashboard \\n \" \"2. Agree with Terms of Service if asked \\n \" \"3. Click in Create Project \\n \" \"4. Put a cool name in your project \\n \" \"5. Hit create \\n \" \"6. Rerun this command with the flag `reauth=True`. \\n \" \" Like `read_table('br_ibge_pib', 'municipios', reauth=True)`\" ) else : raise e","title":"read_sql()"},{"location":"py_reference_api/#basedosdados.download.read_table","text":"Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False limit int Optional. Number of rows to read from table. None Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download.py def read_table ( dataset_id , table_id , query_project_id = \"basedosdados\" , billing_project_id = None , limit = None , reauth = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. limit (int): Optional. Number of rows to read from table. Returns: pd.DataFrame: Query result \"\"\" if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , reauth = reauth )","title":"read_table()"},{"location":"py_reference_api/#classes","text":"","title":"Classes"},{"location":"py_reference_api/#basedosdados.storage","text":"","title":"basedosdados.storage"},{"location":"py_reference_api/#basedosdados.storage.Storage","text":"Manage files on Google Cloud Storage.","title":"Storage"},{"location":"py_reference_api/#basedosdados.storage.Storage.copy_table","text":"Copies table from a source bucket to your bucket, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests.","title":"copy_table()"},{"location":"py_reference_api/#basedosdados.storage.Storage.copy_table--todo-auto-divides-into-multiple-requests","text":"Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name(str) Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) required mode str Optional Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. #TODO: auto divides into multiple requests Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name(str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Optional Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } \" ) ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) with self . client [ \"storage_staging\" ] . batch (): for blob in source_table_ref : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket )","title":"TODO: auto divides into multiple requests"},{"location":"py_reference_api/#basedosdados.storage.Storage.delete_file","text":"Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"raw\" , \"staging\" ] else : mode = [ mode ] for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists (): blob . delete () elif not_found_ok : return else : blob . delete ()","title":"delete_file()"},{"location":"py_reference_api/#basedosdados.storage.Storage.delete_table","text":"Deletes a table from storage, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests.","title":"delete_table()"},{"location":"py_reference_api/#basedosdados.storage.Storage.delete_table--todo-auto-divides-into-multiple-requests","text":"Parameters: Name Type Description Default mode str Optional Folder of which dataset to update. 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. If your request requires more than 1000 blobs, you should divide it in multiple requests. #TODO: auto divides into multiple requests Args: mode (str): Optional Folder of which dataset to update. bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = self . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) else : table_blobs = self . bucket . list_blobs ( prefix = prefix ) with self . client [ \"storage_staging\" ] . batch (): for blob in table_blobs : self . delete_file ( filename = str ( blob . name ) . replace ( prefix , \"\" ), mode = mode , not_found_ok = not_found_ok , )","title":"TODO: auto divides into multiple requests"},{"location":"py_reference_api/#basedosdados.storage.Storage.init","text":"Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) else : self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" )","title":"init()"},{"location":"py_reference_api/#basedosdados.storage.Storage.upload","text":"Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 3 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 3 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"raw\" , \"staging\" ] else : mode = [ mode ] for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise Exception ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"Set if_exists to 'replace' to overwrite data\" )","title":"upload()"},{"location":"py_reference_api/#basedosdados.dataset","text":"","title":"basedosdados.dataset"},{"location":"py_reference_api/#basedosdados.dataset.Dataset","text":"Manage datasets in BigQuery.","title":"Dataset"},{"location":"py_reference_api/#basedosdados.dataset.Dataset.create","text":"Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflic exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflic exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ]) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : job = m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. except Conflict : if if_exists == \"pass\" : return else : raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) # Make prod dataset public self . publicize ()","title":"create()"},{"location":"py_reference_api/#basedosdados.dataset.Dataset.delete","text":"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True )","title":"delete()"},{"location":"py_reference_api/#basedosdados.dataset.Dataset.init","text":"Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) for file in ( Path ( self . templates ) / \"dataset\" ) . glob ( \"*\" ): if file . name in [ \"dataset_config.yaml\" , \"README.md\" ]: # Load and fill template template = self . _render_template ( f \"dataset/ { file . name } \" , dict ( dataset_id = self . dataset_id ) ) # Write file ( self . dataset_folder / file . name ) . open ( \"w\" ) . write ( template ) # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self","title":"init()"},{"location":"py_reference_api/#basedosdados.dataset.Dataset.publicize","text":"Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' Source code in basedosdados/dataset.py def publicize ( self , mode = \"all\" ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ])","title":"publicize()"},{"location":"py_reference_api/#basedosdados.dataset.Dataset.update","text":"Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' Source code in basedosdados/dataset.py def update ( self , mode = \"all\" ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. dataset = m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ]), fields = [ \"description\" ] ) # Make an API request.","title":"update()"},{"location":"py_reference_api/#basedosdados.table","text":"","title":"basedosdados.table"},{"location":"py_reference_api/#basedosdados.table.Table","text":"Manage tables in Google Cloud Storage and BigQuery.","title":"Table"},{"location":"py_reference_api/#basedosdados.table.Table.append","text":"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Source code in basedosdados/table.py def append ( self , filepath , partitions = None , if_exists = \"raise\" , ** upload_args ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing \"\"\" Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = None , if_exists = if_exists , ** upload_args , ) self . create ( if_exists = \"replace\" )","title":"append()"},{"location":"py_reference_api/#basedosdados.table.Table.create","text":"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> , for instance, year=2012/country=BR Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery None partitioned bool Optional. Whether data is partitioned False if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Implement if_table_exists=raise * Implement if_table_exists=pass Source code in basedosdados/table.py def create ( self , path = None , job_config_params = None , partitioned = False , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>`, for instance, `year=2012/country=BR` Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery partitioned (bool): Optional. Whether data is partitioned if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Implement if_table_exists=raise * Implement if_table_exists=pass \"\"\" # Add data to storage if isinstance ( path , ( str , PosixPath , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , ) external_config = external_config = bigquery . ExternalConfig ( \"CSV\" ) external_config . options . skip_leading_rows = 1 external_config . options . allow_quoted_newlines = True external_config . options . allow_jagged_rows = True external_config . autodetect = False external_config . schema = self . _load_schema ( \"staging\" ) external_config . source_uris = ( f \"gs:// { self . bucket_name } /staging/ { self . dataset_id } / { self . table_id } /*\" ) if partitioned : hive_partitioning = bigquery . external_config . HivePartitioningOptions () hive_partitioning . mode = \"AUTO\" hive_partitioning . source_uri_prefix = self . uri . format ( dataset = self . dataset_id , table = self . table_id ) . replace ( \"*\" , \"\" ) external_config . hive_partitioning = hive_partitioning table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = external_config table_ref = self . client [ \"bigquery_staging\" ] . list_tables ( f \" { self . dataset_id } _staging\" ) table_names = [ table . table_id for table in table_ref ] if self . table_id in table_names : if if_table_exists == \"pass\" : return None elif if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table )","title":"create()"},{"location":"py_reference_api/#basedosdados.table.Table.delete","text":"Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging|all] required Source code in basedosdados/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging|all] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True )","title":"delete()"},{"location":"py_reference_api/#basedosdados.table.Table.init","text":"Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing required Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) elif if_folder_exists == \"pass\" : return self partition_columns = [] if isinstance ( data_sample_path , ( str , PosixPath , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == \".csv\" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] if data_sample_path . suffix == \".csv\" : columns = next ( csv . reader ( open ( data_sample_path , \"r\" ))) else : raise NotImplementedError ( \"Data sample just supports comma separated csv files\" ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass else : raise FileExistsError ( f \"No config files found at { self . table_folder } \" ) else : if if_table_config_exists == \"raise\" : if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) else : self . _make_template ( columns , partition_columns ) if if_table_config_exists == \"replace\" : self . _make_template ( columns , partition_columns ) return self","title":"init()"},{"location":"py_reference_api/#basedosdados.table.Table.publish","text":"Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" ) . read () ) self . update ( \"prod\" )","title":"publish()"},{"location":"py_reference_api/#basedosdados.table.Table.update","text":"Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found True Source code in basedosdados/table.py def update ( self , mode = \"all\" , not_found_ok = True ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) if mode == \"all\" : mode = [ \"prod\" , \"staging\" ] else : mode = [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue table . description = self . _render_template ( \"table/table_description.txt\" , self . table_config ) # save table description open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , ) . write ( table . description ) # if m == \"prod\":/ table . schema = self . _load_schema ( m ) self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = [ \"description\" , \"schema\" ] )","title":"update()"},{"location":"style_data/","text":"Diretrizes para valores em c\u00e9lulas Todas os dados na BD+ devem seguir o mesmo padr\u00e3o de formatos e unidades de medida para que haja uma real integra\u00e7\u00e3o. Formatos Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Porcentagem: entre 0-100 Unidades de medida \u00c1rea: 1 km2 Temperatura: graus Celsius Dinheiro: 1 unidade Popula\u00e7\u00e3o: 1 pessoa","title":"Valores"},{"location":"style_data/#diretrizes-para-valores-em-celulas","text":"Todas os dados na BD+ devem seguir o mesmo padr\u00e3o de formatos e unidades de medida para que haja uma real integra\u00e7\u00e3o.","title":"Diretrizes para valores em c\u00e9lulas"},{"location":"style_data/#formatos","text":"Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Porcentagem: entre 0-100","title":"Formatos"},{"location":"style_data/#unidades-de-medida","text":"\u00c1rea: 1 km2 Temperatura: graus Celsius Dinheiro: 1 unidade Popula\u00e7\u00e3o: 1 pessoa","title":"Unidades de medida"},{"location":"style_naming_columns/","text":"Diretrizes para nomes de colunas Nomear colunas \u00e9 parte fundamental da constru\u00e7\u00e3o do nosso reposit\u00f3rio. Escolhas feitas nesse momento tem ramifica\u00e7\u00f5es longe no futuro, e por isso levamos essa parte muito a s\u00e9rio. Um bom nome de coluna satisfaz algumas condi\u00e7\u00f5es : Respeita padr\u00f5es das tabelas de diret\u00f3rios; \u00c9 o mais intuitivo, claro, e extenso poss\u00edvel; Tem todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ ; Segue os padr\u00f5es BD+ abaixo. Padr\u00f5es BD+ de nomea\u00e7\u00e3o de colunas Usar nomes j\u00e1 presentes no reposit\u00f3rio. Alguns exemplos: ano mes id_municipio sigla_uf idade cargo resultado votos receita despesa preco Evitar abrevia\u00e7\u00f5es . Exemplos para evitar: qtde , n . Exce\u00e7\u00f5es naturais: resid (resid\u00eancia), ocor (ocorr\u00eancia). Lista de prefixos comuns id_ nome_ data_ numero_ quantidade_ prop_ taxa_ razao_ indice_ indicador_ tipo_ sigla_ sequencial_ Lista de sufixos comuns _pc (per capita) TODO: definir exatamente id como identificadores INT64 Pensou em melhorias para os padr\u00f5es definidos? Abra um issue no nosso Github ou mande uma mensagem para conversarmos :)","title":"Colunas"},{"location":"style_naming_columns/#diretrizes-para-nomes-de-colunas","text":"Nomear colunas \u00e9 parte fundamental da constru\u00e7\u00e3o do nosso reposit\u00f3rio. Escolhas feitas nesse momento tem ramifica\u00e7\u00f5es longe no futuro, e por isso levamos essa parte muito a s\u00e9rio. Um bom nome de coluna satisfaz algumas condi\u00e7\u00f5es : Respeita padr\u00f5es das tabelas de diret\u00f3rios; \u00c9 o mais intuitivo, claro, e extenso poss\u00edvel; Tem todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ ; Segue os padr\u00f5es BD+ abaixo.","title":"Diretrizes para nomes de colunas"},{"location":"style_naming_columns/#padroes-bd-de-nomeacao-de-colunas","text":"Usar nomes j\u00e1 presentes no reposit\u00f3rio. Alguns exemplos: ano mes id_municipio sigla_uf idade cargo resultado votos receita despesa preco Evitar abrevia\u00e7\u00f5es . Exemplos para evitar: qtde , n . Exce\u00e7\u00f5es naturais: resid (resid\u00eancia), ocor (ocorr\u00eancia). Lista de prefixos comuns id_ nome_ data_ numero_ quantidade_ prop_ taxa_ razao_ indice_ indicador_ tipo_ sigla_ sequencial_ Lista de sufixos comuns _pc (per capita) TODO: definir exatamente id como identificadores INT64","title":"Padr\u00f5es BD+ de nomea\u00e7\u00e3o de colunas"},{"location":"style_naming_columns/#pensou-em-melhorias-para-os-padroes-definidos","text":"Abra um issue no nosso Github ou mande uma mensagem para conversarmos :)","title":"Pensou em melhorias para os padr\u00f5es definidos?"},{"location":"style_naming_datasets_tables/","text":"Diretrizes para nomes de bases e tabelas As bases devem ser organizadas no BigQuery de maneira consistente , que permita uma busca f\u00e1cil e intuitiva , e seja escal\u00e1vel . As diretrizes definidas para nomenclatura das bases ( datasets ) e tabelas ( tables ) est\u00e3o descritas abaixo. O BigQuery permite busca por nome da tabela, base ou r\u00f3tulo ( label ), incluindo correspond\u00eancias parciais. Base ( dataset ) Tabela ( table ) Mundial mundo_<instituicao>_<descricao> <descricao> Federal <pais_sigla>_<instituicao>_<descricao> <descricao> Estadual <pais_sigla>_<estado_sigla>_<instituicao>_<descricao> <descricao> Municipal <pais_sigla>_<estado_sigla>_<cidade>_<instituicao>_<descricao> <descricao> Os componentes dos nomes s\u00e3o: mundo/pais_sigla/estado_sigla/cidade : Abrang\u00eancia da organiza\u00e7\u00e3o - e n\u00e3o os dados (ex: IBGE tem abrang\u00eancia br ) organiza\u00e7\u00e3o : Nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais. descri\u00e7\u00e3o : Nome descritivo e \u00fanico para cada tabela e dataset (nome da tabela basta ser \u00fanico dentro do datatset ). Se atente ao modo de escrita Utilize somente letras min\u00fasculas Remova acentos, pontua\u00e7\u00f5es e espa\u00e7os Separe palavras por _ N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br-rj-detran-rj ) Exemplos Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.pnad_microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnad.pnad_cont\u00ednua_microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ). Temas Os temas listados abaixo s\u00e3o os mesmos do mecanismo de busca da Base dos Dados, mas temas podem ser adicionados e trocados. O BigQuery permite que sejam adicionados r\u00f3tulos ( labels ) \u00e0s tabelas, que tamb\u00e9m funcionam para a busca de dados. Os r\u00f3tulos ainda n\u00e3o s\u00e3o obrigat\u00f3rios, sugerimos a busca pela institui\u00e7\u00e3o ou descri\u00e7\u00e3o dos dados. Tema Identificador (r\u00f3tulo) Agropecu\u00e1ria agropecuaria Ci\u00eancia, Tecnologia e Inova\u00e7\u00e3o ciencia-tec-inov Cultura e Arte cultura-arte Diversidade e Inclus\u00e3o diversidade Educa\u00e7\u00e3o educacao Energia energia Esportes esportes Economia economia Governo e Finan\u00e7as P\u00fablicas gov-fin-pub Hist\u00f3ria historia Infraestrutura e Transportes infra-transp Jornalismo e Comunica\u00e7\u00e3o comunicacao Meio Ambiente meio-ambiente Justi\u00e7a justica Organiza\u00e7\u00e3o Territorial territorio Pol\u00edtica politica Popula\u00e7\u00e3o populacao Religi\u00e3o religiao Seguran\u00e7a, Crime, Viol\u00eancia e Conflito seguranca Sa\u00fade saude Turismo turismo Urbaniza\u00e7\u00e3o urbanizacao","title":"Bases e tabelas"},{"location":"style_naming_datasets_tables/#diretrizes-para-nomes-de-bases-e-tabelas","text":"As bases devem ser organizadas no BigQuery de maneira consistente , que permita uma busca f\u00e1cil e intuitiva , e seja escal\u00e1vel . As diretrizes definidas para nomenclatura das bases ( datasets ) e tabelas ( tables ) est\u00e3o descritas abaixo. O BigQuery permite busca por nome da tabela, base ou r\u00f3tulo ( label ), incluindo correspond\u00eancias parciais. Base ( dataset ) Tabela ( table ) Mundial mundo_<instituicao>_<descricao> <descricao> Federal <pais_sigla>_<instituicao>_<descricao> <descricao> Estadual <pais_sigla>_<estado_sigla>_<instituicao>_<descricao> <descricao> Municipal <pais_sigla>_<estado_sigla>_<cidade>_<instituicao>_<descricao> <descricao> Os componentes dos nomes s\u00e3o: mundo/pais_sigla/estado_sigla/cidade : Abrang\u00eancia da organiza\u00e7\u00e3o - e n\u00e3o os dados (ex: IBGE tem abrang\u00eancia br ) organiza\u00e7\u00e3o : Nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais. descri\u00e7\u00e3o : Nome descritivo e \u00fanico para cada tabela e dataset (nome da tabela basta ser \u00fanico dentro do datatset ). Se atente ao modo de escrita Utilize somente letras min\u00fasculas Remova acentos, pontua\u00e7\u00f5es e espa\u00e7os Separe palavras por _ N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br-rj-detran-rj )","title":"Diretrizes para nomes de bases e tabelas"},{"location":"style_naming_datasets_tables/#exemplos","text":"Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.pnad_microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnad.pnad_cont\u00ednua_microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ).","title":"Exemplos"},{"location":"style_naming_datasets_tables/#temas","text":"Os temas listados abaixo s\u00e3o os mesmos do mecanismo de busca da Base dos Dados, mas temas podem ser adicionados e trocados. O BigQuery permite que sejam adicionados r\u00f3tulos ( labels ) \u00e0s tabelas, que tamb\u00e9m funcionam para a busca de dados. Os r\u00f3tulos ainda n\u00e3o s\u00e3o obrigat\u00f3rios, sugerimos a busca pela institui\u00e7\u00e3o ou descri\u00e7\u00e3o dos dados. Tema Identificador (r\u00f3tulo) Agropecu\u00e1ria agropecuaria Ci\u00eancia, Tecnologia e Inova\u00e7\u00e3o ciencia-tec-inov Cultura e Arte cultura-arte Diversidade e Inclus\u00e3o diversidade Educa\u00e7\u00e3o educacao Energia energia Esportes esportes Economia economia Governo e Finan\u00e7as P\u00fablicas gov-fin-pub Hist\u00f3ria historia Infraestrutura e Transportes infra-transp Jornalismo e Comunica\u00e7\u00e3o comunicacao Meio Ambiente meio-ambiente Justi\u00e7a justica Organiza\u00e7\u00e3o Territorial territorio Pol\u00edtica politica Popula\u00e7\u00e3o populacao Religi\u00e3o religiao Seguran\u00e7a, Crime, Viol\u00eancia e Conflito seguranca Sa\u00fade saude Turismo turismo Urbaniza\u00e7\u00e3o urbanizacao","title":"Temas"},{"location":"support/","text":"Nos apoie Financiamento coletivo \ud83d\udcb8 A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"In\u00edcio"},{"location":"support/#nos-apoie","text":"","title":"Nos apoie"},{"location":"support/#financiamento-coletivo","text":"A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes","title":"Financiamento coletivo \ud83d\udcb8"},{"location":"support/#ajude-a-manter-nosso-codigo","text":"Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb"}]}